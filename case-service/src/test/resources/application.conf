# Default application.conf runs the Case Service with
#  Cassandra as storage for events and
#  Postgres as storage for the projections to enable the Case queries and
#  Akka as the runtime system
# All of these can be configured through this conf file.
# In addition many settings can also be passed as environment variables

akka {
  loglevel = INFO
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  actor {
    serialize-messages = on

    serializers {
      cafienne_serializer = "org.cafienne.akka.actor.serialization.CafienneSerializer"
    }

    serialization-bindings {
      "org.cafienne.akka.actor.serialization.CafienneSerializable" = cafienne_serializer
    }
  }

  persistence {
    journal {
      # Using Dennis Vriend in-mem journal
      plugin = "inmemory-journal"
      auto-start-journals = ["inmemory-journal"]
    }
  }
}

cafienne {
  # Platform has owners that are allowed to create/disable/enable tenants
  #  This property specifies the set of user-id's that are owners
  #  This array may not be empty.
  platform {
    owners = ["admin"]
    owners = ${?CAFIENNE_PLATFORM_OWNERS}
    default-tenant = "world"
    default-tenant = ${?CAFIENNE_PLATFORM_DEFAULT_TENANT}
  }

  engine {
    # Properties for sending tasks of type Mail
    mail-service {
      # Here you can fill any regular javax.mail properties
      #  All properties mentioned here are passed into the connection with the mail server
      mail.host = mailcatcher
      mail.smtp.port = 1025
      # Optional username/pwd to be used to connect to the mail server
      authentication {
        user = ""
        password = ""
      }
    }

    timer-service {
      # The number of seconds that the TimerService should wait to persist it's snapshot
      # after a timer has occurred or has been canceled.
      # Note: for setting timers always immediately snapshots are stored (better safe than sorry)
      persist-delay = 10
    }

    platform-service {
      # Number of threads to push platform updates to cases and tenants
      workers = 10
      # The number of seconds that the PlatformService should wait to persist it's snapshots
      persist-delay = 5
    }
  }

  api {
    bindhost = "localhost"
    bindport = 2027

    security {
      # configuration settings for OpenID Connect
      oidc {
        connect-url = "http://localhost:5556/dex/.well-known/openid-configuration"
        connect-url = ${?CAFIENNE_OIDC_CONNECT_URL}
        token-url = "http://127.0.0.1:5556/dex/token"
        token-url = ${?CAFIENNE_OIDC_TOKEN_URL}
        key-url = "http://127.0.0.1:5556/dex/keys"
        key-url = ${?CAFIENNE_OIDC_KEY_URL}
        authorization-url = "http://127.0.0.1:5556/dex/auth"
        authorization-url = ${?CAFIENNE_OIDC_AUTHORIZATION_URL}
        issuer = "http://localhost:5556/dex"
        issuer = ${?CAFIENNE_OIDC_ISSUER}

        ##################################################################################
        ##  Below settings can be used to configure to the untrustworthy token service  ##
        ##   Only use this to run the Cafienne Testscript Framework for test purposes.  ##
        ##   Never use this in production systems.                                      ##
        ##################################################################################
        #connect-url = "http://localhost:2377/.well-known/openid-configuration"
        #token-url = "http://localhost:2377/token"
        #key-url = "http://localhost:2377/keys"
        #authorization-url = "http://localhost:2377/auth"
        #issuer = "Cafienne Test Framework"
      }

      # The subject of a valid JWT token is used to query the corresponging registered platform user from the database.
      # These identities can be cached to avoid repeated queries and thereby improve throughput times.
      # The size of the cache can be set here, it defaults to 1000
      # The cache is disabled if size is 0 or a negative number.
      identity.cache.size = 1000

      ###################################################################################################
      ##                                                                                               ##
      ## Fill this setting to true to allow developers to access engine events without authentication  ##
      ##                                                                                               ##
      ##   WARNING - Enabling opens up the full engine in read-only mode for anyone to access          ##
      ##                                                                                               ##
      ###################################################################################################
      debug.events.open = false
      debug.events.open = ${?CAFIENNE_DEBUG_EVENTS}
    }
  }

  # The case engine supports various ways to list, load and deploy case definitions.
  # The below settings can be used to configure various options; default settings use
  # a file based definition provider.
  # An alternative is to use the StartCaseDefinitionProvider, in which
  # case definitions must be passed along with the StartCase REST API itself.
  #
  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    # Default provider is based on reading/writing from the local file system
    provider = "org.cafienne.cmmn.repository.file.FileBasedDefinitionProvider"
    location = "./definitions"
    location =  ${?CAFIENNE_CMMN_DEFINITIONS_PATH}
    cache {
      size = 100
    }

    # Use the below provider to start cases while passing the definition along the StartCase call
    #  Note that the StartCaseDefinitionProvider also makes use of the same cache settings
    # provider = "org.cafienne.cmmn.repository.StartCaseDefinitionProvider"
  }

  actor {
    # the seconds of idle time after which a case actor is removed from akka memory
    # if the case has not received new commands after the specified number of seconds,
    # the case engine will ask akka to remove the case from memory to avoid memory leaks.
    idle-period = 600

    # If debug is true, then all StartCase commands by default will run in debug mode,
    #  unless specified otherwise in the command
    debug = false
  }

  # This setting tells cafienne which journal to use for reading events.
  #  If omitted, cafienne will try to guess the read journal, based on the akka settings
  read-journal = "inmemory-read-journal"

  query-db {
    profile = "slick.jdbc.HsqldbProfile$"
    db {
      driver = "org.hsqldb.jdbc.JDBCDriver"
      url = "jdbc:hsqldb:mem:mymemdb"
      # User name to connect, update and query
      user = "SA"
      password = ""
      # User name for migration of schema upon startup
      migrateUser = "SA"
      migratePwd = ""
      numThreads = 10
      connectionTimeout = 5000
      validationTimeout = 5000
    }

    # Configuration options handling exceptions that may occur while reading
    #  the event streams that populate the query-db tables
    #  See also https://doc.akka.io/docs/akka/current/stream/stream-error.html#restart-with-backoff
    restart-stream {
        min-back-off = 500ms
        max-back-off = 30s
        random-factor = 0.20
        max-restarts = 20
    }
  }
}

inmemory-journal {
  event-adapters {
    tagging = "org.cafienne.akka.actor.tagging.CaseTaggingEventAdapter"
  }

  event-adapter-bindings {
    "org.cafienne.akka.actor.event.ModelEvent" = tagging
  }
}

inmemory-read-journal {
  # Absolute path to the write journal plugin configuration section to get the event adapters from
  write-plugin = "inmemory-journal"

  # ask timeout on Futures
  ask-timeout = "10s"

  # New events are retrieved (polled) with this interval.
  refresh-interval = "100ms"

  # How many events to fetch in one query (replay) and keep buffered until they
  # are delivered downstreams.
  max-buffer-size = "100"
}