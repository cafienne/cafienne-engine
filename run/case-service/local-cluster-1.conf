##################################################################################################
##                                                                                              ##
## Default configurations to use PostgreSQL both for storing events and projection information  ##
##                                                                                              ##
##  Some of the settings can be passed as environment variables                                 ##
##                                                                                              ##
##################################################################################################
pekko {
  loglevel = INFO
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
  logger-startup-timeout = 10s

  cluster.jmx.enabled = on

  actor {
    provider = cluster
    # default-dispatcher {
    #   # Throughput for default Dispatcher, set to 1 for as fair as possible
    #   throughput = 1
    # }

    processtask-dispatcher {
      type = Dispatcher
      executor = "thread-pool-executor"
      thread-pool-executor {
        fixed-pool-size = 64
      }
      throughput = 1
    }

    localrouter-mailbox {
      mailbox-type = "org.apache.pekko.dispatch.SingleConsumerOnlyUnboundedMailbox"
      mailbox-capacity = 1500
    }

    deployment {
      "/default-router" {
        mailbox = "pekko.actor.localrouter-mailbox"
      }

      "/cases" {
        mailbox = "pekko.actor.localrouter-mailbox"
      }

      "/process-tasks/*" {
        dispatcher = "pekko.actor.processtask-dispatcher"
      }
    }

    serialize-messages = on

    serializers {
      cafienne_serializer = "org.cafienne.infrastructure.serialization.CafienneSerializer"
      jackson-json = "org.apache.pekko.serialization.jackson.JacksonJsonSerializer"
    }

    serialization-bindings {
      "org.cafienne.infrastructure.serialization.CafienneSerializable" = cafienne_serializer
      "org.cafienne.infrastructure.serialization.JacksonSerializable" = jackson-json
    }
  }

  remote.artery {
    canonical.port = 7345
    canonical.hostname = 127.0.0.1
  }

  cluster {
    seed-nodes = [
      "pekko://Cafienne-Case-System@127.0.0.1:7345",
      "pekko://Cafienne-Case-System@127.0.0.1:7355"]

    sharding {
      number-of-shards = 100
      passivation {
        # Disable passivation, as ModelActorMonitor takes care of that
        strategy = "none"
      }
    }

    downing-provider-class = "org.apache.pekko.cluster.sbr.SplitBrainResolverProvider"

    log-info-verbose = on
  }

  http {
   
    host-connection-pool {
      max-connections = 10000
      max-open-requests = 6168
    }

    server {
      pipelining-limit = 1024
      max-connections = 4096
      backlog = 2048
      request-timeout = 240s
    }
  }

  persistence {
    journal {
      plugin = "jdbc-journal"
      auto-start-journals = ["jdbc-journal"]
    }
    snapshot-store {
      plugin = "jdbc-snapshot-store"
      # Enable the line below to automatically start the snapshot-store when the actorsystem is started
      auto-start-snapshot-stores = []
    }
  }
}

cafienne {
  # Engine wide platform settings
  platform {
    # Platform has owners that are allowed to create/disable/enable tenants
    #  This property specifies the set of user-id's that are owners
    #  This array may not be empty.
    owners = ["admin"]
    owners = ${?CAFIENNE_PLATFORM_OWNERS}
    # Default tenant will be used when a user does not provide the tenant as a parameter to
    #  the API call (e.g. in StartCase). When the user is member of only one tenant,
    #  then that tenant will be submitted along with the StartCase command.
    #  If the user belongs to multiple tenants, then this default-tenant option will be passed.
    default-tenant = "world"
    default-tenant = ${?CAFIENNE_PLATFORM_DEFAULT_TENANT}
    # bootstrap-file holds a reference to a json or yaml file that has default tenant information.
    #  E.g., tenant name, tenant owners, tenant users can be given in this file.
    #  During launch of the case engine, the file will be scanned and a special CreateTenant command is sent
    #  into the system, thereby setting up a default tenant with owners and users.
    #  The bootstrap configuration will search for this file, and try to parse it into a standard akka Config
    #  object.
    # If the bootstrap-file property is not filled, the system will search for a file that holds
    #  the default tenant name plus either a .conf, .json, .yml or .yaml extension.
    #  In case of tenant 'world', the system would search for existence in the following order:
    #  - 'world.conf'
    #  - 'world.json'
    #  - 'world.yml'
    #  - 'world.yaml'
    # If none of these files are found, the bootstrap attempt will be skipped.
    bootstrap-file = "world.conf"
  }

  engine {
      timer-service {
      # Reference to the akka journal JDBC configuration (typically a shared-database).
      # This is used by the Timer Service to store the events. Note, this MUST point to the akka journal database,
      # as that is also the database where the flyway JDBC schema migrations are applied to.
      store = "pekko-persistence-jdbc.shared-databases.slick"
    }
    storage-service {
      # Configuration of the archive mechanism - where to archive to and restore from
      archive {
        plugin = "file"

        file {
          directory = "./archive1"
        }
      }

      # By default, when the engine starts, it checks whether any running storage processes
      #  have to be recovered. This can be disabled through this property.
      disable-recovery = false
    }
  }

  # If debug is true, then all StartCase commands by default will run in debug mode,
  #  unless specified otherwise in the command
  debug = true

  api {
    bindhost = 0.0.0.0 
    bindport = 2027

    security {
      # configuration settings for OpenID Connect
      oidc {
        ### This one seems to have dynamic resolution ...
        ###   but since the key-url is filled, that will take precedence (at this moment)
        connect-url = "http://localhost:2377"
        token-url = "http://localhost:2377/token"
        key-url = "http://localhost:2377/keys"
        authorization-url = "http://localhost:2377/auth"
        issuer = "http://localhost:2377"
      }

      ###################################################################################################
      ##                                                                                               ##
      ## Fill this setting to true to allow developers to access engine events without authentication  ##
      ##                                                                                               ##
      ##   WARNING - Enabling opens up the full engine in read-only mode for anyone to access          ##
      ##                                                                                               ##
      ###################################################################################################
      debug.events.open = true
      debug.events.open = ${?CAFIENNE_DEBUG_EVENTS}
    }
  }

  # The case engine reads definitions as XML files from disk and/or the classpath.
  # The files are cached in-memory, based on their lastModified timestamp
  # (i.e., if you change a file on disk, the engine will reload it into the cache).
  # By default, the engine will read from the configured location. If the definitions file cannot be found
  # in this location, the engine will try to load it as a resource from the classpath, hence enabling to ship
  # fixed definitions in a jar file.
  definitions {
    provider = "org.cafienne.cmmn.repository.file.FileBasedDefinitionProvider"
    //provider = "org.cafienne.cmmn.repository.StartCaseDefinitionProvider"
    location = "./definitions"
    location =  ${?CAFIENNE_CMMN_DEFINITIONS_PATH}
    cache {
      size = 100
    }
  }

  actor {
    # the seconds of idle time after which a case actor is removed from akka memory
    # if the case has not received new commands after the specified number of seconds,
    # the case engine will ask akka to remove the case from memory to avoid memory leaks.
    idle-period = 600
    ask-timeout = 120
  }

  read-journal = "jdbc-read-journal"

  persistence {
    initialize-database-schema = true
    #table-prefix = "casemanagement"

    event-db {
      schema-history-table = "event_schema_history"
    }

    query-db {
      schema-history-table = "query_schema_history"

      # This setting tells cafienne which journal to use for reading events.
      #  If this omitted, cafienne will try to guess the read journal, based on the akka settings
      profile = "slick.jdbc.PostgresProfile$"
      profile = ${?PROJECTION_DB_PROFILE}
      db {
        registerMbeans = true
        poolName = "queryDb"
        driver = "org.postgresql.Driver"
        driver = ${?PROJECTION_DB_DRIVER}
        ###################################################################
        ##                                                               ##
        ##  Database schema 'cafienne-query' must be created manually    ##
        ##                                                               ##
        ###################################################################
        url = "jdbc:postgresql://localhost:5432/cluster1?reWriteBatchedInserts=true"
        url = ${?PROJECTION_DB_URL}

        ###################################################################
        ##                                                               ##
        ##  MAKE SURE TO FILL USER AND PASSWORD FOR CONNECTION           ##
        ##                                                               ##
        ###################################################################
        user = "postgres"
        user = ${?PROJECTION_DB_USER}
        password = "postgres"
        password = ${?PROJECTION_DB_PASSWORD}
        numThreads = 100
        maxConnections = 100
        minConnections = 10
        connectionTimeout = 5000
        validationTimeout = 5000
      }

      # Configuration options handling exceptions that may occur while reading
      #  the event streams that populate the query-db tables
      #  See also https://doc.akka.io/docs/akka/current/stream/stream-error.html#restart-with-backoff
      restart-stream {
        min-back-off = 500ms
        max-back-off = 30s
        random-factor = 0.20
        max-restarts = 20
        max-restarts-within = 15m
      }
    }
  }
}

#######################################################################################
##                                                                                   ##
##  Below are settings for Postgres journal db                                       ##
##                                                                                   ##
##    As of now, the database schema 'cluster1' must be created manually             ##
##                                                                                   ##
#######################################################################################

pekko-persistence-jdbc {
  shared-databases {
    slick {
      profile = "slick.jdbc.PostgresProfile$"
      profile = ${?EVENT_DB_PROFILE}
      db {
        driver = "org.postgresql.Driver"
        driver = ${?EVENT_DB_DRIVER}
        url = "jdbc:postgresql://localhost:5432/cluster1?reWriteBatchedInserts=true"
        url = ${?EVENT_DB_URL}
        # User name to connect, update and query
        user = "postgres"
        user = ${?EVENT_DB_USER}
        password = "postgres"
        password = ${?EVENT_DB_PASSWORD}
        numThreads = 5
        connectionTimeout = 5000
        validationTimeout = 5000
        maxConnections = 5
        minConnections = 1
      }
    }
  }
}

jdbc-journal {
  use-shared-db = "slick"
}

jdbc-snapshot-store {
  use-shared-db = "slick"
}

jdbc-read-journal {
  use-shared-db = "slick"
}
